{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# AI-Generated Zoology\n",
    "## An implementation of Image-to-Image Translation\n",
    "### Main file\n",
    "#### For project in Pattern Recognition, COMP 472\n",
    "#### Due December 10th, 2020\n",
    "#### Use of Python 3.8 (see requirement.txt)\n",
    "#### By Sandra Buchen (2631798)\n",
    "#### Nigel Yong Sao Young (40089856) \n",
    "#### Dan Raileanu (40019882) \n",
    "#### InÃ©s Gonzalez Pepe (40095696) \n",
    "#### Marc Vicuna (40079109)\n",
    "This main file has the capacity to run the project from top to bottom. Read the markdown cells for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITZuApL56Mny"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
    "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the constants\n",
    "These constants were chosen for our implementation, they may vary on other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CbTEt448b4R"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Dan\\source\\AI-Generated-Zoology\\data/\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 3\n",
    "PATH = os.path.join(os.getcwd(), 'data/')\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "If you are not interested in utility functions, just instantiate all functions until the next subtitle markdown text.\n",
    "Most of the data manipulation is handled by these utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO9ZAGH5K3SY"
   },
   "outputs": [],
   "source": [
    "# Loads the the image_file as an input_image and a real_image representing the target\n",
    "def load_input_target(image_file):\n",
    "\n",
    "    #read\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    #reformat\n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "    real_image = image[:, :w, :]\n",
    "    input_image = image[:, w:, :]\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the image_file as a single input_image\n",
    "def load_input(image_file):\n",
    "\n",
    "   #read\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    input_image = tf.cast(image, tf.float32)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loading, IO test\n",
    "IO is important for this project. Make sure you have already downloaded the data. \n",
    "See README.md if there is any issue. This test should display the first image of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "height": 558
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3959,
     "status": "ok",
     "timestamp": 1551645018835
    },
    "id": "4OLHMpsQ5aOv",
    "outputId": "5bfde74b-a0dd-42fa-8e9c-4f6eb42215ed"
   },
   "outputs": [],
   "source": [
    "# # Loading image\n",
    "# inp, re = load(PATH+'train/1.jpeg')\n",
    "# # casting to int for matplotlib to show the image\n",
    "# plt.figure()\n",
    "# plt.imshow(inp/255)\n",
    "# plt.figure()\n",
    "# plt.imshow(re/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwwYQpu9FzDu"
   },
   "outputs": [],
   "source": [
    "# Resizing the image\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width], \n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width], \n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yn3IwqhiIszt"
   },
   "outputs": [],
   "source": [
    "# Cropping the image using Tensorflow's utility functions\n",
    "def random_crop(input_image, real_image):\n",
    "    \n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop( stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    \n",
    "    return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muhR2cgbLKWW"
   },
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image  = (real_image   / 127.5) - 1\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVQOjcPVLrUc"
   },
   "outputs": [],
   "source": [
    "# Implementing the random jitter (see below for details)\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "    \n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Random Jitter visually\n",
    "Random jitter is a small, low-cost preprocessing step used \n",
    "in the context of Image-to-Image translation for natural images,\n",
    "insensitive to pixel shift and mirroring. Using the prior \n",
    "knowledge of natural images, it encourages better generalizability \n",
    "of the model. <br>\n",
    "Random jittering as described in the paper is to:\n",
    "* Resize an image to bigger height and width\n",
    "* Randomnly crop to the original size\n",
    "* Randomnly flip the image horizontally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "height": 390
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1551645021485
    },
    "id": "n0OGdi6D92kM",
    "outputId": "00242d5c-9efe-498d-fab0-68f47161b074"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n# Plotting 4 times the same image with random_jitter applied\\nplt.figure(figsize=(6, 6))\\nfor i in range(4):\\n    rj_inp, rj_re = random_jitter(inp, re)  \\n    plt.subplot(2, 2, i+1)\\n    plt.imshow(rj_inp/255.0)\\n    plt.axis('off')\\nplt.show()\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plotting 4 times the same image with random_jitter applied\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)  \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(rj_inp/255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyaP4hLJ8b4W"
   },
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, use of random jitter and normalization\n",
    "def load_image_train(image_file):\n",
    "    \n",
    "    input_image, real_image = load_input_target(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VB3Z6D_zKSru"
   },
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, adapted to testing\n",
    "def load_image_test(image_file):\n",
    "    \n",
    "    input_image, real_image = load_input_target(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, adapted to predicting without target image\n",
    "def load_image_predict(image_file):\n",
    "    input_image= load_input(image_file)\n",
    "    input_image, real_image = resize(input_image, input_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "    input_image, real_image = normalize(input_image, input_image)\n",
    "    \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Input Pipeline\n",
    "Setting up the input Pipeline for training. Make sure all your data is directly in the train directory, in jpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "# Pipeline setup for training\n",
    "train_dataset = tf.data.Dataset.list_files(PATH+'train(cats)/*.jpeg')\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "# Pipeline setup for training\n",
    "test_dataset = tf.data.Dataset.list_files(PATH+'test(cats)/*.jpeg')\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Build the Generator\n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "# Downsampling, implementation of the encoder.\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # 1st layer, Conv\n",
    "    result.add( tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "    # 2nd layer, Batchnorm\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    # 3rd layer, Leaky ReLU\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "# Upsampling, implementation of the decoder.\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # 1st layer, Conv\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "    # 2nd layer, Batchnorm\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    # 3rd layer, Dropout (Randomization)\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    # 4th layer, regular ReLU\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "def Generator():\n",
    "    # Downsampling stack\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "        downsample(128, 4), # (bs, 64, 64, 128)\n",
    "        downsample(256, 4), # (bs, 32, 32, 256)\n",
    "        downsample(512, 4), # (bs, 16, 16, 512)\n",
    "        downsample(512, 4), # (bs, 8, 8, 512)\n",
    "        downsample(512, 4), # (bs, 4, 4, 512)\n",
    "        downsample(512, 4), # (bs, 2, 2, 512)\n",
    "        downsample(512, 4), # (bs, 1, 1, 512)\n",
    "    ]\n",
    "    # Upsampling stack\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "        upsample(256, 4), # (bs, 32, 32, 512)\n",
    "        upsample(128, 4), # (bs, 64, 64, 256)\n",
    "        upsample(64, 4), # (bs, 128, 128, 128)\n",
    "    ]\n",
    "    # Initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, \n",
    "                                         strides=2, \n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "    concat = tf.keras.layers.Concatenate() \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
    "    x = inputs\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = concat([x, skip])\n",
    "    \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Generator on 1 image\n",
    "You should be able to see an image composed of noise, with the trace of your first edge image. Ignore the warning if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1551645027026
    },
    "id": "U1N1_obwtdQH",
    "outputId": "67deedd8-6493-4b7e-e721-7b34290361db"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "#gen_output = generator(inp[tf.newaxis,...], training=False)\n",
    "#plt.imshow(gen_output[0,...]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "## Build the Discriminator\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU).\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1).\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake. \n",
    "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "def Discriminator():\n",
    "    \n",
    "    # Initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n",
    "    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "    \n",
    "    # Downsampling blocks instantiation\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "    \n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "    # 1st layer, Conv\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1, \n",
    "                                kernel_initializer=initializer, \n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "    # 2nd layer, Batchnorm\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    # 3rd layer, Leaky ReLU\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "    # 4th layer, Conv\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Discriminator on 1 image\n",
    "You should be able to see an image composed of noise, with the trace of your first edge image. Ignore the warning if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1551645028164
    },
    "id": "gDkA05NE6QMs",
    "outputId": "cdeef0a1-a913-4c40-be6d-7a30f35e03cd"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "#disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
    "#plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "#plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ede4p2YELFa"
   },
   "source": [
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "  \n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator $loss = GAN_{loss} + \\lambda * L1_{loss}$, where $\\lambda = 100$. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1Xbz5OaLj5C"
   },
   "outputs": [],
   "source": [
    "# General loss instantiation\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    \n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "# Defining the generator loss\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    LAMBDA = 100\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "# Instantiating optimizers\n",
    "generator_optimizer      = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKUZnDiqQrAh"
   },
   "source": [
    "## Checkpoints (Object-based saving)\n",
    "Creates a new checkpoint, for your new model. <br>\n",
    "Do not modify the format of the checkpoint. If you do, modify the following cell corresponding to your new format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "#Directory\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "# Loading\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Training\n",
    "\n",
    "* We start by iterating over the dataset\n",
    "* The generator gets the input image and we get a generated output.\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "\n",
    "\n",
    "## Generate Images\n",
    "\n",
    "* After training, its time to generate some images!\n",
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output we expect.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "# Generates images based on the current model\n",
    "def generate_images(model, test_input, tar=None, savePath = None):\n",
    "    \"\"\"\n",
    "    The training=True is intentional here since we want the batch \n",
    "    statistics while running the model on the test dataset. If we \n",
    "    use training=False, we will get the accumulated statistics \n",
    "    learned from the training dataset (which we don't want).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    # With ground truth or not\n",
    "    if tar != None:\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    else:\n",
    "        display_list = [test_input[0], prediction[0]]\n",
    "        title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    if savePath != None:\n",
    "        plt.savefig(savePath)\n",
    "    else:\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "# Trains on a single image. The function depends on the instantiation of \n",
    "# many functions and objects, make sure you ran through all cells.\n",
    "@tf.function\n",
    "def train_step(input_image, target):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "        \n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "    \n",
    "    generator_gradients = gen_tape.gradient(gen_loss, \n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, \n",
    "                                          discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, \n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n",
    "                                              discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "# Trains on all the dataset\n",
    "def train(dataset, epochs):  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for input_image, target in dataset:\n",
    "            train_step(input_image, target)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        for inp, tar in test_dataset.take(1):\n",
    "            generate_images(generator, inp, tar)\n",
    "        # Saving (checkpoint) the model every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0: checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        # Output to console. Trust me, it takes a while. Always good to have some sign of life.\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model on 1 image\n",
    "You should be able to see the edge image, the real image and the predicted image, composed of noise, with the trace of your first edge image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1551658750610
    },
    "id": "a1zZmKmvOH85",
    "outputId": "adf5f083-4543-4326-abec-fe28f9b91b09"
   },
   "outputs": [],
   "source": [
    "train(train_dataset, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test\n",
    "Loads the last checkpoint, to load the trained model. <br>\n",
    "Verify you have downloaded the lastest checkpoint. This is the trained version of the model. After the data, it should be close to the most expensive file memory-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Testing on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUgSnmy2nqSP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the entire test dataset\n",
    "for i ,(inp, tar) in enumerate(test_dataset):\n",
    "    generate_images(generator, inp, tar, \"{}{}{}.jpeg\".format(PATH, 'results/', i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom images - Dogs Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/Dog/'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.data.Dataset.list_files('custom/Dog/*.jpeg')\n",
    "predict_dataset = predict_dataset.map(load_image_predict)\n",
    "predict_dataset = predict_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in predict_dataset:\n",
    "    generate_images(generator, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom images - Cats Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/Cat/'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.data.Dataset.list_files('custom/Cat/*.jpeg')\n",
    "predict_dataset = predict_dataset.map(load_image_predict)\n",
    "predict_dataset = predict_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in predict_dataset:\n",
    "    generate_images(generator, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "pix2pix.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}