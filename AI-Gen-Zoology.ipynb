{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# AI-Generated Zoology: An implementation of Image-to-Image Translation\n",
    "\n",
    "### Main file\n",
    "\n",
    "- Project for Pattern Recognition, COMP 473\n",
    "-  Use of Python 3.8 (see requirement.txt)\n",
    "\n",
    "#### Members\n",
    "+ Sandra Buchen (2631798)\n",
    "+ Nigel Yong Sao Young (40089856) \n",
    "+ Dan Raileanu (40019882) \n",
    "+ In√©s Gonzalez Pepe (40095696) \n",
    "+ Marc Vicuna (40079109)\n",
    "\n",
    "This main file has the capacity to run the project from top to bottom. Read the markdown cells for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITZuApL56Mny"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "import random\n",
    "import shutil\n",
    "tf.__version__ \n",
    "\n",
    "# Random seed for reproducibility\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the constants\n",
    "These constants were chosen for our implementation, they may vary on other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CbTEt448b4R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dan\\source\\AI-Generated-Zoology\\data/\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 3\n",
    "PATH = os.path.join(os.getcwd(), 'data/')\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "Used to setup the training/test data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_h(im1, im2):\n",
    "    \"\"\"stacks two images horizontally\"\"\"\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "\n",
    "def resize_combine(raw_path, sketch_path, result_path, img_size):\n",
    "    \"\"\"resizes and merges matching images from raw_path and sketch_path and outputs new image in result_path\"\"\"\n",
    "    #make sure that RESULT_PATH is an empty dir\n",
    "    if os.path.isdir(result_path):\n",
    "        assert len(os.listdir(result_path)) == 0, result_path + \" is not empty, clear it first\"\n",
    "    else:\n",
    "        os.mkdir(result_path)\n",
    "\n",
    "    raw_dir = os.listdir(raw_path)\n",
    "    sketch_dir = os.listdir(sketch_path)\n",
    "    assert len(raw_dir) == len(sketch_dir), '{} and {} have different number of files'.format(raw_path, sketch_path)\n",
    "    result_dir = zip(raw_dir, sketch_dir)\n",
    "\n",
    "\n",
    "    for raw_file, sketch_file in result_dir:\n",
    "        assert raw_file.split('.')[0] == sketch_file.split('.')[0], 'raw_file: ' + raw_file + \"  and sketch_file: \" + sketch_file\\\n",
    "            +\"  don't have matching names. Maybe clear \" + sketch_path + \" and run PhotoSketch model again\"\n",
    "        raw_img = Image.open('{}{}'.format(raw_path, raw_file))\n",
    "        raw_img = raw_img.resize(img_size)\n",
    "        sketch_img = Image.open('{}{}'.format(sketch_path, sketch_file))\n",
    "        sketch_img = sketch_img.resize(img_size)\n",
    "        \n",
    "        merged_img = get_concat_h(raw_img, sketch_img)\n",
    "        merged_img.save('{}{}{}'.format(result_path, raw_file.split('.')[0], '.jpeg'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_into_train_test(input_path, train_path, test_path, test_ratio, clear_dir=False):\n",
    "    \"\"\"\n",
    "    shuffles samples randomly into train and test set\n",
    "    set clear_dir to True if you want it to clear any existing files in the train/test folders\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(input_path), input_path + ' does not exist'\n",
    "    assert test_ratio >=0 and test_ratio<=1, \"TEST_RATIO must be between 0 and 1\"\n",
    "    for dir_path in [train_path, test_path]:\n",
    "        if os.path.isdir(dir_path):\n",
    "            if clear_dir and len(os.listdir(dir_path)) != 0:\n",
    "                for filename in os.listdir(dir_path):\n",
    "                    os.remove(dir_path+filename)\n",
    "            else:\n",
    "                assert len(os.listdir(dir_path)) == 0, dir_path + \" is not empty, clear it first or set CLEAR_DIR to True\"\n",
    "        else:\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "    input_dir = os.listdir(input_path)\n",
    "    random.shuffle(input_dir)\n",
    "\n",
    "    for i,filename in enumerate(input_dir):\n",
    "        if i <= test_ratio * len(input_dir):\n",
    "            shutil.copy(input_path+filename, test_path+filename)\n",
    "        else:\n",
    "            shutil.copy(input_path+filename, train_path+filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix model\n",
    "Run cell below to load all functions for pix2pix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    #####################\n",
    "        Pix2Pix model  \n",
    "    #####################\n",
    "    \"\"\"\n",
    "    def downsample(self, filters, size, apply_batchnorm=True):\n",
    "        \"\"\"Downsampling, implementation of the encoder.\"\"\"\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        result = tf.keras.Sequential()\n",
    "        # 1st layer, Conv\n",
    "        result.add( tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "        # 2nd layer, Batchnorm\n",
    "        if apply_batchnorm:\n",
    "            result.add(tf.keras.layers.BatchNormalization())\n",
    "        # 3rd layer, Leaky ReLU\n",
    "        result.add(tf.keras.layers.LeakyReLU())\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def upsample(self, filters, size, apply_dropout=False):\n",
    "        \"\"\"Upsampling, implementation of the decoder.\"\"\"\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        result = tf.keras.Sequential()\n",
    "        # 1st layer, Conv\n",
    "        result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "        # 2nd layer, Batchnorm\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        # 3rd layer, Dropout (Randomization)\n",
    "        if apply_dropout:\n",
    "            result.add(tf.keras.layers.Dropout(0.5))\n",
    "        # 4th layer, regular ReLU\n",
    "        result.add(tf.keras.layers.ReLU())\n",
    "        return result\n",
    "\n",
    "\n",
    "    def Generator(self):\n",
    "        \"\"\"Defining the generator\"\"\"\n",
    "        # Downsampling stack\n",
    "        down_stack = [\n",
    "            self.downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "            self.downsample(128, 4), # (bs, 64, 64, 128)\n",
    "            self.downsample(256, 4), # (bs, 32, 32, 256)\n",
    "            self.downsample(512, 4), # (bs, 16, 16, 512)\n",
    "            self.downsample(512, 4), # (bs, 8, 8, 512)\n",
    "            self.downsample(512, 4), # (bs, 4, 4, 512)\n",
    "            self.downsample(512, 4), # (bs, 2, 2, 512)\n",
    "            self.downsample(512, 4), # (bs, 1, 1, 512)\n",
    "        ]\n",
    "        # Upsampling stack\n",
    "        up_stack = [\n",
    "            self.upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "            self.upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "            self.upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "            self.upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "            self.upsample(256, 4), # (bs, 32, 32, 512)\n",
    "            self.upsample(128, 4), # (bs, 64, 64, 256)\n",
    "            self.upsample(64, 4), # (bs, 128, 128, 128)\n",
    "        ]\n",
    "        # Initialization\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, \n",
    "                                            strides=2, \n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer,\n",
    "                                            activation='tanh') # (bs, 256, 256, 3)\n",
    "        concat = tf.keras.layers.Concatenate() \n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
    "        x = inputs\n",
    "        # Downsampling through the model\n",
    "        skips = []\n",
    "        for down in down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        \n",
    "        skips = reversed(skips[:-1])\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = concat([x, skip])\n",
    "        \n",
    "        x = last(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "    def Discriminator(self):\n",
    "        \"\"\"Defining the discriminator\"\"\"\n",
    "        # Initialization\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n",
    "        x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "        \n",
    "        # Downsampling blocks instantiation\n",
    "        down1 = self.downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "        down2 = self.downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "        down3 = self.downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "        \n",
    "        zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "        # 1st layer, Conv\n",
    "        conv = tf.keras.layers.Conv2D(512, 4, strides=1, \n",
    "                                    kernel_initializer=initializer, \n",
    "                                    use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "        # 2nd layer, Batchnorm\n",
    "        batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "        # 3rd layer, Leaky ReLU\n",
    "        leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "        zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "        # 4th layer, Conv\n",
    "        last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                    kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "        \n",
    "        return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Model constructor\"\"\"\n",
    "    def __init__(self):\n",
    "        #setup generator and discriminator\n",
    "        self.generator = self.Generator()\n",
    "        self.discriminator = self.Discriminator()\n",
    "        # General loss instantiation\n",
    "        self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        # Optimizers instantiation\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        # Datasets\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.predict_dataset = None\n",
    "\n",
    "\n",
    "\n",
    "    def load_input_target(self, image_file):\n",
    "        \"\"\"Loads the the image_file as an input_image and a real_image representing the target\"\"\"\n",
    "        #read\n",
    "        image = tf.io.read_file(image_file)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "        \n",
    "        #reformat to  return input/target separetely\n",
    "        w = tf.shape(image)[1]\n",
    "        w = w // 2\n",
    "        real_image = image[:, :w, :]\n",
    "        input_image = image[:, w:, :]\n",
    "        \n",
    "        input_image = tf.cast(input_image, tf.float32)\n",
    "        real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "        return input_image, real_image\n",
    "\n",
    "\n",
    "    def load_input(self, image_file):\n",
    "        \"\"\" Loads the image_file as a single input_image\"\"\"\n",
    "        #read\n",
    "        image = tf.io.read_file(image_file)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "        \n",
    "        input_image = tf.cast(image, tf.float32)\n",
    "\n",
    "        return input_image\n",
    "\n",
    "\n",
    "    def resize(self, input_image, real_image, height, width):\n",
    "        \"\"\"Resize the images to given height,width\"\"\"\n",
    "        input_image = tf.image.resize(input_image, [height, width], \n",
    "                                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        real_image = tf.image.resize(real_image, [height, width], \n",
    "                                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        \n",
    "        return input_image, real_image\n",
    "\n",
    "\n",
    "    def random_crop(self, input_image, real_image):\n",
    "        \"\"\"Cropping the images using Tensorflow's utility functions\"\"\"        \n",
    "        stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "        cropped_image = tf.image.random_crop( stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "        \n",
    "        return cropped_image[0], cropped_image[1]\n",
    "\n",
    "\n",
    "    def normalize(self, input_image, real_image):\n",
    "        \"\"\"Normalizing the images to [-1, 1]\"\"\"\n",
    "        input_image = (input_image / 127.5) - 1\n",
    "        real_image  = (real_image   / 127.5) - 1\n",
    "        \n",
    "        return input_image, real_image\n",
    "\n",
    "\n",
    "    @tf.function()\n",
    "    def random_jitter(self, input_image, real_image):\n",
    "        \"\"\"\n",
    "        As mentioned in the paper, we apply random jittering and mirroring to the training dataset.\n",
    "        *In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
    "        *In random mirroring, the image is randomly flipped horizontally i.e left to right.\n",
    "        \"\"\"\n",
    "        # resizing to 286 x 286 x 3\n",
    "        input_image, real_image = self.resize(input_image, real_image, 286, 286)\n",
    "        \n",
    "        # randomly cropping to 256 x 256 x 3\n",
    "        input_image, real_image = self.random_crop(input_image, real_image)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            # random mirroring\n",
    "            input_image = tf.image.flip_left_right(input_image)\n",
    "            real_image = tf.image.flip_left_right(real_image)\n",
    "        \n",
    "        return input_image, real_image\n",
    "\n",
    "    def load_image_train(self, image_file):\n",
    "        \"\"\"Loading the image with heavier preprocessing, use of random jitter and normalization\"\"\"\n",
    "        input_image, real_image = self.load_input_target(image_file)\n",
    "        input_image, real_image = self.random_jitter(input_image, real_image)\n",
    "        input_image, real_image = self.normalize(input_image, real_image)\n",
    "        \n",
    "        return input_image, real_image\n",
    "\n",
    "\n",
    "    def load_image_test(self, image_file):\n",
    "        \"\"\"Loading the image with heavier preprocessing, adapted to testing\"\"\"\n",
    "        \n",
    "        input_image, real_image = self.load_input_target(image_file)\n",
    "        input_image, real_image = self.resize(input_image, real_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "        input_image, real_image = self.normalize(input_image, real_image)\n",
    "        \n",
    "        return input_image, real_image\n",
    "\n",
    "\n",
    "    def load_image_predict(self, image_file):\n",
    "        \"\"\"Loading the image with heavier preprocessing, adapted for predicting without target image\"\"\"\n",
    "        input_image= self.load_input(image_file)\n",
    "        input_image, real_image = self.resize(input_image, input_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "        input_image, real_image = self.normalize(input_image, input_image)\n",
    "        \n",
    "        return input_image\n",
    "\n",
    "    def load_train(self, train_path):\n",
    "        \"\"\"Pipeline setup for training\"\"\"\n",
    "        self.train_dataset = tf.data.Dataset.list_files(PATH+train_path+'*.jpeg')\n",
    "        self.train_dataset = self.train_dataset.shuffle(BUFFER_SIZE)\n",
    "        self.train_dataset = self.train_dataset.map(self.load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        self.train_dataset = self.train_dataset.batch(1)\n",
    "\n",
    "\n",
    "    def load_test(self, test_path):\n",
    "        \"\"\"Pipeline setup for testing\"\"\"\n",
    "        self.test_dataset = tf.data.Dataset.list_files(PATH+test_path+'*.jpeg')\n",
    "        self.test_dataset = self.test_dataset.shuffle(BUFFER_SIZE)\n",
    "        self.test_dataset = self.test_dataset.map(self.load_image_test)\n",
    "        self.test_dataset = self.test_dataset.batch(1)\n",
    "\n",
    "\n",
    "    def load_predict(self, pred_path):\n",
    "        \"\"\"Pipeline setup for predictions without target image\"\"\"\n",
    "        self.predict_dataset = tf.data.Dataset.list_files(PATH+pred_path+'*.jpeg')\n",
    "        self.predict_dataset = self.predict_dataset.map(self.load_image_test)\n",
    "        self.predict_dataset = self.predict_dataset.batch(1)\n",
    "\n",
    "\n",
    "    def discriminator_loss(self, disc_real_output, disc_generated_output):\n",
    "        \"\"\"Defining the discriminator loss\"\"\"        \n",
    "        real_loss = self.loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        generated_loss = self.loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "        total_disc_loss = real_loss + generated_loss\n",
    "        \n",
    "        return total_disc_loss\n",
    "\n",
    "\n",
    "    def generator_loss(self, disc_generated_output, gen_output, target):\n",
    "        \"\"\"Defining the generator loss\"\"\"\n",
    "        LAMBDA = 100\n",
    "        gan_loss = self.loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "        \n",
    "        # mean absolute error\n",
    "        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "        total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "        \n",
    "        return total_gen_loss\n",
    "\n",
    "\n",
    "    def generate_images(self, model, test_input, tar=None, savePath = None):\n",
    "        \"\"\"\n",
    "        Generates images based on the current model.\n",
    "        The training=True is intentional here since we want the batch \n",
    "        statistics while running the model on the test dataset. If we \n",
    "        use training=False, we will get the accumulated statistics \n",
    "        learned from the training dataset (which we don't want).\n",
    "        \"\"\"     \n",
    "        # Prediction\n",
    "        prediction = model(test_input, training=True)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(15,15))\n",
    "        \n",
    "        # With ground truth or not\n",
    "        if tar != None:\n",
    "            display_list = [test_input[0], tar[0], prediction[0]]\n",
    "            title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "        else:\n",
    "            display_list = [test_input[0], prediction[0]]\n",
    "            title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i+1)\n",
    "            plt.title(title[i])\n",
    "            # getting the pixel values between [0, 1] to plot it.\n",
    "            plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "        if savePath != None:\n",
    "            plt.savefig(savePath)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input_image, target):\n",
    "        \"\"\"Trains on a single image\"\"\"\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_output = self.generator(input_image, training=True)\n",
    "            \n",
    "            disc_real_output = self.discriminator([input_image, target], training=True)\n",
    "            disc_generated_output = self.discriminator([input_image, gen_output], training=True)\n",
    "            gen_loss = self.generator_loss(disc_generated_output, gen_output, target)\n",
    "            disc_loss = self.discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        \n",
    "        generator_gradients = gen_tape.gradient(gen_loss, \n",
    "                                            self.generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, \n",
    "                                            self.discriminator.trainable_variables)\n",
    "        \n",
    "        self.generator_optimizer.apply_gradients(zip(generator_gradients, \n",
    "                                            self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n",
    "                                                self.discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs):\n",
    "        assert self.train_dataset != None, \"No train_dataset present. First call load_train()\"\n",
    "        \"\"\"Trains on all the train_dataset samples for epochs number of times\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            for input_image, target in self.train_dataset:\n",
    "                self.train_step(input_image, target)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            for inp, tar in self.test_dataset.take(1):\n",
    "                self.generate_images(self.generator, inp, tar)\n",
    "            # Saving checkpoint for the model every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0: self.save_checkpoint()\n",
    "            # Output to console prediction on some train data for each epoch\n",
    "            print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    \n",
    "    def test(self, num):\n",
    "        assert self.test_dataset != None, \"No test_dataset present. First call load_test()\"\n",
    "        for i, (inp, tar) in enumerate(self.test_dataset.take(num)):\n",
    "                self.generate_images(self.generator, inp, tar, \"{}{}{}.jpeg\".format(PATH, 'results/', i))\n",
    "\n",
    "\n",
    "    def predict(self, num):\n",
    "        assert self.predict_dataset != None, \"No predict_dataset present. First call load_predict()\"\n",
    "        for i, inp in enumerate(self.predict_dataset.take(num)):\n",
    "                self.generate_images(self.generator, inp, tar=None, savePath=\"{}{}{}.jpeg\".format(PATH, 'results/', i))\n",
    "\n",
    "\n",
    "    def show_Generator_Discriminator(self, num):\n",
    "        for inp, tar in self.train_dataset.take(num):\n",
    "            fig,axs = plt.subplots(1,4, figsize=(12,3))\n",
    "\n",
    "            axs[0].imshow(inp[0])\n",
    "            axs[0].set_title(\"Input\")\n",
    "\n",
    "            axs[1].imshow(tar[0])\n",
    "            axs[1].set_title(\"Real\")\n",
    "\n",
    "            gen_output = self.generator(inp, training=False)\n",
    "            plt.figure()\n",
    "            axs[2].imshow(gen_output[0,...]);\n",
    "            axs[2].set_title(\"Generator\")\n",
    "\n",
    "            disc_out = self.discriminator([inp, gen_output], training=False)\n",
    "            axs[3].imshow(disc_out[0,...,-1], cmap='RdBu_r')\n",
    "            axs[3].set_title(\"Discriminator\")\n",
    "\n",
    "\n",
    "    def set_checkpoint_dir(self, path):\n",
    "        \"\"\"Set path for loading/saving checkpoints\"\"\"\n",
    "        self.checkpoint_dir = path\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,\n",
    "                                        discriminator_optimizer=self.discriminator_optimizer,\n",
    "                                        generator=self.generator,\n",
    "                                        discriminator=self.discriminator)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save checkpoint in checkpoint_dir folder\"\"\"\n",
    "        self.checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "                \n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Restoring the latest checkpoint in checkpoint_dir\"\"\"\n",
    "        self.checkpoint.restore(tf.train.latest_checkpoint(self.checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model from scratch (optional)\n",
    "To speed up the process, only a dozen pictures have been placed in data/raw-img/ directory with their coresponding sketch in data/sketch-img\n",
    "Run the cells below for a demo on how our model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "data/resized_combined/ is not empty, clear it first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e92911419ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#resize and combine the raw data with the sketches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresize_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/raw-img/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msketch_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/sketch-img/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/resized_combined/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#shuffle data into training and testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mshuffle_into_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/resized_combined/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/train/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data/test/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclear_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8a71d1e23e69>\u001b[0m in \u001b[0;36mresize_combine\u001b[1;34m(raw_path, sketch_path, result_path, img_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#make sure that RESULT_PATH is an empty dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" is not empty, clear it first\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: data/resized_combined/ is not empty, clear it first"
     ]
    }
   ],
   "source": [
    "#resize and combine the raw data with the sketches\n",
    "resize_combine(raw_path=\"data/raw-img/\", sketch_path=\"data/sketch-img/\", result_path=\"data/resized_combined/\", img_size=(256,256))\n",
    "#shuffle data into training and testing\n",
    "shuffle_into_train_test(input_path=\"data/resized_combined/\", train_path=\"data/train/\", test_path=\"data/test/\", test_ratio=0.2, clear_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciate a model and load train/test paths\n",
    "DemoModel = Model()\n",
    "DemoModel.load_train(\"train/\")\n",
    "DemoModel.load_test(\"test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DemoModel.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DemoModel.show_Generator_Discriminator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DemoModel.test(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DogsModel = Model()\n",
    "DogsModel.load_train(\"train(dogs_small)/\")#loading smaller version of the dogs training set for demo\n",
    "DogsModel.load_test(\"test(dogs)/\")\n",
    "#DogsModel.load_predict(\"custom/Dog/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes about 30sec\n",
    "DogsModel.train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DogsModel.test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DogsModel.show_Generator_Discriminator(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint for 500 epochs of training on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DogsModel.set_checkpoint_dir('training_checkpoints/dogs/')\n",
    "DogsModel.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DogsModel.test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatsModel = Model()\n",
    "CatsModel.load_train(\"train(cats_small)/\")#loading smaller version of the cats training set for demo\n",
    "CatsModel.load_test(\"test(cats)/\")\n",
    "#CatsModel.load_predict(\"custom/Cat/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes about 30sec\n",
    "CatsModel.train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatsModel.show_Generator_Discriminator(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatsModel.test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint for 500 epochs of training on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatsModel.set_checkpoint_dir('training_checkpoints/cats/')\n",
    "CatsModel.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatsModel.test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO9ZAGH5K3SY"
   },
   "outputs": [],
   "source": [
    "# Loads the the image_file as an input_image and a real_image representing the target\n",
    "def load_input_target(image_file):\n",
    "\n",
    "    #read\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    #reformat\n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "    real_image = image[:, :w, :]\n",
    "    input_image = image[:, w:, :]\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the image_file as a single input_image\n",
    "def load_input(image_file):\n",
    "\n",
    "   #read\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    input_image = tf.cast(image, tf.float32)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loading, IO test\n",
    "IO is important for this project. Make sure you have already downloaded the data. \n",
    "See README.md if there is any issue. This test should display the first image of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 558
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3959,
     "status": "ok",
     "timestamp": 1551645018835
    },
    "id": "4OLHMpsQ5aOv",
    "outputId": "5bfde74b-a0dd-42fa-8e9c-4f6eb42215ed"
   },
   "outputs": [],
   "source": [
    "# # Loading image\n",
    "# inp, re = load(PATH+'train/1.jpeg')\n",
    "# # casting to int for matplotlib to show the image\n",
    "# plt.figure()\n",
    "# plt.imshow(inp/255)\n",
    "# plt.figure()\n",
    "# plt.imshow(re/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwwYQpu9FzDu"
   },
   "outputs": [],
   "source": [
    "# Resizing the image\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width], \n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width], \n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yn3IwqhiIszt"
   },
   "outputs": [],
   "source": [
    "# Cropping the image using Tensorflow's utility functions\n",
    "def random_crop(input_image, real_image):\n",
    "    \n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop( stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    \n",
    "    return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muhR2cgbLKWW"
   },
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image  = (real_image   / 127.5) - 1\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVQOjcPVLrUc"
   },
   "outputs": [],
   "source": [
    "# Implementing the random jitter (see below for details)\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "    \n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Random Jitter visually\n",
    "Random jitter is a small, low-cost preprocessing step used \n",
    "in the context of Image-to-Image translation for natural images,\n",
    "insensitive to pixel shift and mirroring. Using the prior \n",
    "knowledge of natural images, it encourages better generalizability \n",
    "of the model. <br>\n",
    "Random jittering as described in the paper is to:\n",
    "* Resize an image to bigger height and width\n",
    "* Randomnly crop to the original size\n",
    "* Randomnly flip the image horizontally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 390
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1551645021485
    },
    "id": "n0OGdi6D92kM",
    "outputId": "00242d5c-9efe-498d-fab0-68f47161b074"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plotting 4 times the same image with random_jitter applied\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)  \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(rj_inp/255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyaP4hLJ8b4W"
   },
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, use of random jitter and normalization\n",
    "def load_image_train(image_file):\n",
    "    \n",
    "    input_image, real_image = load_input_target(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VB3Z6D_zKSru"
   },
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, adapted to testing\n",
    "def load_image_test(image_file):\n",
    "    \n",
    "    input_image, real_image = load_input_target(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image with heavier preprocessing, adapted to predicting without target image\n",
    "def load_image_predict(image_file):\n",
    "    input_image= load_input(image_file)\n",
    "    input_image, real_image = resize(input_image, input_image,  IMG_HEIGHT, IMG_WIDTH) \n",
    "    input_image, real_image = normalize(input_image, input_image)\n",
    "    \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Input Pipeline\n",
    "Setting up the input Pipeline for training. Make sure all your data is directly in the train directory, in jpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "# Pipeline setup for training\n",
    "train_dataset = tf.data.Dataset.list_files(PATH+'train(cats_small)/*.jpeg')\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "# Pipeline setup for training\n",
    "test_dataset = tf.data.Dataset.list_files(PATH+'test(cats)/*.jpeg')\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Build the Generator\n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "# Downsampling, implementation of the encoder.\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # 1st layer, Conv\n",
    "    result.add( tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "    # 2nd layer, Batchnorm\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    # 3rd layer, Leaky ReLU\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "# Upsampling, implementation of the decoder.\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # 1st layer, Conv\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "    # 2nd layer, Batchnorm\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    # 3rd layer, Dropout (Randomization)\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    # 4th layer, regular ReLU\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "def Generator():\n",
    "    # Downsampling stack\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "        downsample(128, 4), # (bs, 64, 64, 128)\n",
    "        downsample(256, 4), # (bs, 32, 32, 256)\n",
    "        downsample(512, 4), # (bs, 16, 16, 512)\n",
    "        downsample(512, 4), # (bs, 8, 8, 512)\n",
    "        downsample(512, 4), # (bs, 4, 4, 512)\n",
    "        downsample(512, 4), # (bs, 2, 2, 512)\n",
    "        downsample(512, 4), # (bs, 1, 1, 512)\n",
    "    ]\n",
    "    # Upsampling stack\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "        upsample(256, 4), # (bs, 32, 32, 512)\n",
    "        upsample(128, 4), # (bs, 64, 64, 256)\n",
    "        upsample(64, 4), # (bs, 128, 128, 128)\n",
    "    ]\n",
    "    # Initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, \n",
    "                                         strides=2, \n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "    concat = tf.keras.layers.Concatenate() \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
    "    x = inputs\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = concat([x, skip])\n",
    "    \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Generator on 1 image\n",
    "You should be able to see an image composed of noise, with the trace of your first edge image. Ignore the warning if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1551645027026
    },
    "id": "U1N1_obwtdQH",
    "outputId": "67deedd8-6493-4b7e-e721-7b34290361db"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "#gen_output = generator(inp[tf.newaxis,...], training=False)\n",
    "#plt.imshow(gen_output[0,...]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "## Build the Discriminator\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU).\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1).\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake. \n",
    "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "def Discriminator():\n",
    "    \n",
    "    # Initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n",
    "    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "    \n",
    "    # Downsampling blocks instantiation\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "    \n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "    # 1st layer, Conv\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1, \n",
    "                                kernel_initializer=initializer, \n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "    # 2nd layer, Batchnorm\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    # 3rd layer, Leaky ReLU\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "    # 4th layer, Conv\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Discriminator on 1 image\n",
    "You should be able to see an image composed of noise, with the trace of your first edge image. Ignore the warning if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1551645028164
    },
    "id": "gDkA05NE6QMs",
    "outputId": "cdeef0a1-a913-4c40-be6d-7a30f35e03cd"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "#disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
    "#plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "#plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ede4p2YELFa"
   },
   "source": [
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "  \n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator $loss = GAN_{loss} + \\lambda * L1_{loss}$, where $\\lambda = 100$. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1Xbz5OaLj5C"
   },
   "outputs": [],
   "source": [
    "# General loss instantiation\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    \n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "# Defining the generator loss\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    LAMBDA = 100\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "# Instantiating optimizers\n",
    "generator_optimizer      = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKUZnDiqQrAh"
   },
   "source": [
    "## Checkpoints (Object-based saving)\n",
    "Creates a new checkpoint, for your new model. <br>\n",
    "Do not modify the format of the checkpoint. If you do, modify the following cell corresponding to your new format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "#Directory\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "# Loading\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Training\n",
    "\n",
    "* We start by iterating over the dataset\n",
    "* The generator gets the input image and we get a generated output.\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "\n",
    "\n",
    "## Generate Images\n",
    "\n",
    "* After training, its time to generate some images!\n",
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output we expect.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "# Generates images based on the current model\n",
    "def generate_images(model, test_input, tar=None, savePath = None):\n",
    "    \"\"\"\n",
    "    The training=True is intentional here since we want the batch \n",
    "    statistics while running the model on the test dataset. If we \n",
    "    use training=False, we will get the accumulated statistics \n",
    "    learned from the training dataset (which we don't want).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    # With ground truth or not\n",
    "    if tar != None:\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    else:\n",
    "        display_list = [test_input[0], prediction[0]]\n",
    "        title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    if savePath != None:\n",
    "        plt.savefig(savePath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "# Trains on a single image. The function depends on the instantiation of \n",
    "# many functions and objects, make sure you ran through all cells.\n",
    "@tf.function\n",
    "def train_step(input_image, target):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "        \n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "    \n",
    "    generator_gradients = gen_tape.gradient(gen_loss, \n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, \n",
    "                                          discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, \n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n",
    "                                              discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "# Trains on all the dataset\n",
    "def train(dataset, epochs):  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for input_image, target in dataset:\n",
    "            train_step(input_image, target)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        for inp, tar in test_dataset.take(1):\n",
    "            generate_images(generator, inp, tar)\n",
    "        # Saving (checkpoint) the model every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0: checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        # Output to console. Trust me, it takes a while. Always good to have some sign of life.\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model on 1 image\n",
    "You should be able to see the edge image, the real image and the predicted image, composed of noise, with the trace of your first edge image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1551658750610
    },
    "id": "a1zZmKmvOH85",
    "outputId": "adf5f083-4543-4326-abec-fe28f9b91b09"
   },
   "outputs": [],
   "source": [
    "train(train_dataset, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test\n",
    "Loads the last checkpoint, to load the trained model. <br>\n",
    "Verify you have downloaded the lastest checkpoint. This is the trained version of the model. After the data, it should be close to the most expensive file memory-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Testing on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUgSnmy2nqSP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the entire test dataset\n",
    "for i ,(inp, tar) in enumerate(test_dataset):\n",
    "    generate_images(generator, inp, tar, \"{}{}{}.jpeg\".format(PATH, 'results/', i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom images - Dogs Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/Dog/'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.data.Dataset.list_files('custom/Dog/*.jpeg')\n",
    "predict_dataset = predict_dataset.map(load_image_predict)\n",
    "predict_dataset = predict_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in predict_dataset:\n",
    "    generate_images(generator, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom images - Cats Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/Cat/'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.data.Dataset.list_files('custom/Cat/*.jpeg')\n",
    "predict_dataset = predict_dataset.map(load_image_predict)\n",
    "predict_dataset = predict_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in predict_dataset:\n",
    "    generate_images(generator, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "pix2pix.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
